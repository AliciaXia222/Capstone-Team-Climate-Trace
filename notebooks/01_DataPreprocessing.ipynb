{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "#ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pycountry\n",
    "import rasterio\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_info_path = \"https://drive.google.com/uc?id=1xfYlruvfAi6yieOd_S69pPYWphckRLr5&export=download\"\n",
    "\n",
    "column_names = [\n",
    "    'Country_Code',        # Alpha-2 code\n",
    "    'ISO_Alpha_3',        # Alpha-3 code\n",
    "    'Numeric_Code',        # Numeric code\n",
    "    'Alpha_2',            # Alpha-2 code (duplicate)\n",
    "    'Country_Name',       # Name of the country\n",
    "    'Capital',            # Capital city\n",
    "    'Area',               # Area in square kilometers\n",
    "    'Population',         # Population\n",
    "    'Region',             # Region\n",
    "    'TLD',                # Top-level domain\n",
    "    'Currency_Code',      # Currency code\n",
    "    'Currency_Name',      # Currency name\n",
    "    'Currency_Numeric',   # Numeric currency code\n",
    "    'Additional_Info'     # Additional information\n",
    "]\n",
    "\n",
    "country_info_df = pd.read_csv(\n",
    "    country_info_path, delimiter=\"\\t\", comment=\"#\", on_bad_lines=\"skip\", header=None\n",
    ")\n",
    "\n",
    "country_info_df.columns = column_names\n",
    "country_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_zip_url = \"https://drive.google.com/uc?id=1UQzdO7suT0BnwKBeNybMG97vM9GIDogA\"\n",
    "countries_zip_file_path = \"../../allCountries.zip\"\n",
    "\n",
    "# Download the ZIP file if it doesn't exist; otherwise, proceed to read the TXT file.\n",
    "if not os.path.exists(countries_zip_file_path):\n",
    "    gdown.download(countries_zip_url, countries_zip_file_path, quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(countries_zip_file_path) as z:\n",
    "    countries_txt_filename = \"allCountries.txt\"\n",
    "\n",
    "    with z.open(countries_txt_filename) as txt_file:\n",
    "        countries_df = pd.read_csv(txt_file, sep=\"\\t\", header=None)\n",
    "\n",
    "\n",
    "# https://download.geonames.org/export/dump/\n",
    "countries_df.columns = [\n",
    "    'geonameid',         \n",
    "    'name',             \n",
    "    'asciiname',        \n",
    "    'alternatenames',  \n",
    "    'latitude',         \n",
    "    'longitude',       \n",
    "    'feature class',    \n",
    "    'feature code',      \n",
    "    'iso alpha 2',      \n",
    "    'cc2',              \n",
    "    'admin1 code',     \n",
    "    'admin2 code',       \n",
    "    'admin3 code',      \n",
    "    'admin4 code',   \n",
    "    'population',      \n",
    "    'elevation',       \n",
    "    'dem',             \n",
    "    'timezone',          \n",
    "    'modification date'  \n",
    "]\n",
    "\n",
    "print(f\"\\nshape: {countries_df.shape}\")\n",
    "countries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eui_url = \"https://drive.google.com/uc?id=12qGq_DLefI1RihIF_RKQUyJtm480-xRC\"\n",
    "eui_df = pd.read_csv(eui_url)\n",
    "\n",
    "print(f\"shape: {eui_df.shape}\")\n",
    "eui_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    countries_df, eui_df, left_on=\"geonameid\", right_on=\"Geonames ID\", how=\"inner\"\n",
    ")\n",
    "assert merged_df.shape[0] == eui_df.shape[0]\n",
    "print(f\"shape: {merged_df.shape}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ISO 3 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding ISO CODE 3\n",
    "alpha_2_to_alpha_3 = {country.alpha_2: country.alpha_3 for country in pycountry.countries}\n",
    "merged_df.loc[:, 'ISO_alpha3'] = merged_df['iso alpha 2'].map(alpha_2_to_alpha_3)\n",
    "\n",
    "# Manually correcting the missing country code for Namibia by assigning 'NAM' because country code is null for Nambia\n",
    "merged_df.loc[merged_df['Country'] == 'Namibia', 'ISO_alpha3'] = 'NAM'\n",
    "merged_df.loc[merged_df['Country'] == 'Namibia', 'iso alpha 2'] = 'NA'\n",
    "\n",
    "print(f\"shape: {merged_df.shape}\")\n",
    "\n",
    "assert merged_df[\"ISO_alpha3\"].isna().sum() == 0, \"There are missing values in the ISO_alpha3 column.\"\n",
    "assert merged_df.shape[0] == 482, \"The number of rows in merged_df is not 482.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_path = '../data/01_raw/population.csv'\n",
    "population_df = pd.read_csv(population_path, skiprows=4)\n",
    "population_2023 = population_df[['Country Code', '2023']]\n",
    "\n",
    "population_2023.rename(columns={\n",
    "    '2023': 'Population_2023',\n",
    "    'Country Code': 'ISO_alpha3'\n",
    "}, inplace=True)\n",
    "\n",
    "#taiwan\n",
    "taiwan_raw = {'Country Name': 'Taiwan', 'ISO_alpha3': 'TWN', 'Population_2023': 23894394}\n",
    "population_2023.loc[len(population_2023)] = taiwan_raw\n",
    "\n",
    "merged_df = merged_df.merge(population_2023, on='ISO_alpha3', how='left')\n",
    "\n",
    "assert merged_df['Population_2023'].notnull().all(), \"Error: There are null values in 'Population_2023'.\"\n",
    "assert merged_df.shape[0] == 482, \"The number of rows in merged_df is not 482.\"\n",
    "print(f\"shape: {merged_df.shape}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDI - Educational Index - Income Index.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDI_EI_II_path = '../data/01_raw/HDI_educationalIndex_incomeIndex.csv'\n",
    "HDI_EI_II_df = pd.read_csv(HDI_EI_II_path)\n",
    "HDI_EI_II_df = HDI_EI_II_df[[\"ISO_Code\", \"Subnational HDI\",\"Educational index\", \"Income index\"]]\n",
    "HDI_EI_II_df.rename(columns={'ISO_Code':'ISO_alpha3' }, inplace=True)\n",
    "\n",
    "\n",
    "merged_df = merged_df.merge(HDI_EI_II_df, \n",
    "                             on='ISO_alpha3', \n",
    "                             how='left')\n",
    "\n",
    "assert merged_df.loc[merged_df['Country'] != 'Taiwan', \n",
    "                     ['Subnational HDI', 'Educational index', 'Income index']].notnull().all().all()\n",
    "assert merged_df.shape[0] == 482, \"The number of rows in merged_df is not 482.\"\n",
    "print(f\"shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_data_path = (\n",
    "    \"../data/01_raw/gdp_data.csv\"\n",
    ")\n",
    "gdp_df = pd.read_csv(gdp_data_path)\n",
    "gdp_df = gdp_df[gdp_df[\"Level\"]==\"National\"]\n",
    "\n",
    "gdp_df = gdp_df[['ISO_Code', '2022']]\n",
    "gdp_df.rename(columns={'2022': 'GDP_2022', 'ISO_Code':'ISO_alpha3' }, inplace=True)\n",
    "\n",
    "\n",
    "merged_df = merged_df.merge(gdp_df, \n",
    "                             on='ISO_alpha3', \n",
    "                             how='left')\n",
    "\n",
    "assert merged_df.loc[merged_df['Country'] != 'Taiwan', \n",
    "                     ['GDP_2022']].notnull().all().all()\n",
    "assert merged_df.shape[0] == 482, \"The number of rows in merged_df is not 482.\"\n",
    "print(f\"shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urbanization rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Urbanization Rate dataset, skipping metadata rows if necessary\n",
    "urbanization_rate_path = (\n",
    "    \"https://drive.google.com/uc?id=1YteyPHAWnJUKG0LWogS98EYnwjRTeZDf&export=download\"\n",
    ")\n",
    "urbanization_rate_df = pd.read_csv(urbanization_rate_path, skiprows=4)\n",
    "\n",
    "\n",
    "urbanization_rate_df = urbanization_rate_df[[\"Country Code\", \"2022\"]].rename(\n",
    "    columns={\"2022\": \"Urbanization_Rate_2022\" , 'Country Code':'ISO_alpha3'}\n",
    ")\n",
    "\n",
    "merged_df = merged_df.merge(urbanization_rate_df, \n",
    "                             on='ISO_alpha3', \n",
    "                             how='left')\n",
    "\n",
    "assert merged_df.shape[0] == 482, \"The number of rows in merged_df is not 482.\"\n",
    "assert merged_df.loc[merged_df['Country'] != 'Taiwan', \n",
    "                     ['Urbanization_Rate_2022']].notnull().all().all()\n",
    "print(f\"shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paris Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_agreement_iso_codes = [\n",
    "    \"AFG\", \"ALB\", \"DZA\", \"AND\", \"AGO\", \"ATG\", \"ARG\", \"AUS\", \"AUT\", \"AZE\", \n",
    "    \"BHS\", \"BHR\", \"BGD\", \"BRB\", \"BLR\", \"BEL\", \"BLZ\", \"BEN\", \"BTN\", \"BOL\", \n",
    "    \"BIH\", \"BWA\", \"BRA\", \"BRN\", \"BGR\", \"BFA\", \"BDI\", \"CPV\", \"KHM\", \"CMR\", \n",
    "    \"CAN\", \"CAF\", \"TCD\", \"CHN\", \"COL\", \"COM\", \"COG\", \"CRI\", \"CIV\", \"HRV\", \n",
    "    \"CUB\", \"CYP\", \"CZE\", \"PRK\", \"COD\", \"DNK\", \"DJI\", \"DMA\", \"DOM\", \"EGY\", \n",
    "    \"SLV\", \"GNQ\", \"ERI\", \"EST\", \"ETH\", \"EUN\", \"FJI\", \"FIN\", \"FRA\", \"GAB\", \n",
    "    \"GEO\", \"DEU\", \"GHA\", \"GRC\", \"GRD\", \"GTM\", \"GIN\", \"GNB\", \"GUY\", \"HTI\", \n",
    "    \"HND\", \"HUN\", \"ISL\", \"IND\", \"IDN\", \"IRN\", \"IRL\", \"ISR\", \"ITA\", \"JAM\", \n",
    "    \"JPN\", \"JOR\", \"KEN\", \"KIR\", \"KWT\", \"LAO\", \"LVA\", \"LBN\", \"LSO\", \"LBR\", \n",
    "    \"LBY\", \"LIE\", \"LTU\", \"LUX\", \"MDG\", \"MYS\", \"MDV\", \"MLI\", \"MLT\", \"MHL\", \n",
    "    \"MUS\", \"MRT\", \"MEX\", \"FSM\", \"MCO\", \"MNG\", \"MNE\", \"MAR\", \"MOZ\", \"MMR\", \n",
    "    \"NAM\", \"NRU\", \"NPL\", \"NLD\", \"NZL\", \"NER\", \"NOR\", \"OMN\", \"PAK\", \"PLW\", \n",
    "    \"PAN\", \"PNG\", \"PRY\", \"PER\", \"PHL\", \"POL\", \"PRT\", \"QAT\", \"KOR\", \"ROU\", \n",
    "    \"RUS\", \"RWA\", \"KNA\", \"LCA\", \"VCT\", \"WSM\", \"SMR\", \"STP\", \"SEN\", \"SRB\", \n",
    "    \"SGP\", \"SVK\", \"SVN\", \"SLB\", \"SOM\", \"ZAF\", \"SSD\", \"ESP\", \"LKA\", \"PSE\", \n",
    "    \"SDN\", \"SUR\", \"SWZ\", \"SWE\", \"CHE\", \"TJK\", \"THA\", \"MKD\", \"TLS\", \"TON\", \n",
    "    \"TTO\", \"TUN\", \"TUR\", \"TUV\", \"UGA\", \"UKR\", \"ARE\", \"GBR\", \"TZA\", \"USA\", \n",
    "    \"URY\", \"VUT\", \"VEN\", \"VNM\", \"ZWE\"\n",
    "]\n",
    "\n",
    "merged_df['Paris_Agreement'] = merged_df['ISO_alpha3'].apply(lambda x: 1 if x in paris_agreement_iso_codes else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_boundaries_url = \"https://drive.google.com/uc?id=1k-2ECd2gwJ9FBz1anMRZy7O85uExAFY_\"\n",
    "world_boundaries_path = \"../../world-administrative-boundaries.geojson\"\n",
    "\n",
    "gdown.download(world_boundaries_url, world_boundaries_path, quiet=False)\n",
    "world_boundaries_df = gpd.read_file(world_boundaries_path)\n",
    "world_boundaries_df = world_boundaries_df[world_boundaries_df['name'] != 'Azores Islands']\n",
    "world_boundaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(world_boundaries_df, left_on='ISO_alpha3', right_on='iso3', how='left')\n",
    "\n",
    "\n",
    "merged_df['Region Grouped'] = np.where(\n",
    "    merged_df['region'] == 'Northern America', \n",
    "    'Northern America',  \n",
    "    np.where(\n",
    "        merged_df['continent'] == 'Americas', \n",
    "        'Central and South America',  \n",
    "        np.where(\n",
    "            merged_df['continent'].isin(['Asia', 'Oceania']), \n",
    "            'Asia & Oceania',  \n",
    "            merged_df['continent']  \n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert merged_df.shape[0] == 482, \"The number of rows in merged_df is not 482.\"\n",
    "assert merged_df['Region Grouped'].notnull().all(), \"The 'Region Grouped' column contains null values.\"\n",
    "merged_df.groupby(['continent', 'region' , 'Region Grouped']).size().reset_index(name='count').sort_values(by='Region Grouped')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDD_path = \"../data/02_interim/HDD.csv\"\n",
    "HDD_df = pd.read_csv(HDD_path)\n",
    "\n",
    "HDD_df = HDD_df.rename(columns={\n",
    "    'total_year': 'hdd_total_year',\n",
    "    'average_year': 'hdd_average_year',\n",
    "    'variance_year': 'hdd_variance_year'\n",
    "})\n",
    "\n",
    "merged_df = merged_df.merge(HDD_df, on='geonameid', how='left')\n",
    "assert merged_df['hdd_total_year'].notnull().all()\n",
    "print(f\"shape: {merged_df.shape}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDD_path = \"../data/02_interim/CDD.csv\"\n",
    "CDD_df = pd.read_csv(HDD_path)\n",
    "\n",
    "CDD_df = CDD_df.rename(columns={\n",
    "    'total_year': 'cdd_total_year',\n",
    "    'average_year': 'cdd_average_year',\n",
    "    'variance_year': 'cdd_variance_year'\n",
    "})\n",
    "\n",
    "merged_df = merged_df.merge(CDD_df, on='geonameid', how='left')\n",
    "assert merged_df['cdd_total_year'].notnull().all()\n",
    "print(f\"shape: {merged_df.shape}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../data/03_processed\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "merged_df.to_csv(os.path.join(output_path, \"merged_df.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
